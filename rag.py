import os
from pathlib import Path
from typing import List
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_openai import OpenAIEmbeddings
from langchain_qdrant import QdrantVectorStore
from qdrant_client import QdrantClient
from langchain_openai import ChatOpenAI
from qdrant_client.models import Distance, VectorParams
from pathlib import Path
from config import OPENAI_API_KEY

# Global variables
embeddings = None
vectorstore = None
llm = None

# URL pour Qdrant (Cloud ou Docker)
QDRANT_URL = os.getenv("QDRANT_URL", None)
QDRANT_CLOUD_URL = os.getenv("QDRANT_CLOUD_URL", None)
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY", None)

def initialize_components():
    """
    Initialise les composants : embeddings, LLM, et Qdrant Cloud.
    """
    global embeddings, llm, vectorstore

    if not os.getenv("OPENAI_API_KEY"):
        raise ValueError("OPENAI_API_KEY not found in environment variables!")

    print("ðŸ”§ Initialisation des composants RAG...")
    
    # 1. CrÃ©er les embeddings OpenAI
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small", openai_api_key=OPENAI_API_KEY)
    print("âœ… Embeddings crÃ©Ã©s")
    
    # 2. CrÃ©er le LLM
    llm = ChatOpenAI(model="gpt-4", temperature=0.1, openai_api_key=OPENAI_API_KEY)
    print("âœ… LLM crÃ©Ã©")

    # 3. CrÃ©er le client Qdrant Cloud avec timeout augmentÃ©
    client = QdrantClient(
        url=QDRANT_CLOUD_URL, 
        api_key=QDRANT_API_KEY,
        timeout=60  # Timeout de 60 secondes au lieu de 5 par dÃ©faut
    )    
    # 4. CrÃ©er le vectorstore LangChain
    vectorstore = QdrantVectorStore(
        client=client,
        collection_name="rag_documents",
        embedding=embeddings
    )
    print("âœ… Vectorstore prÃªt")
    
    return "âœ… Components initialized successfully!"

def rag_agent_with_sources_conversational(query: str, chat_history: list = None):
    """
    Agent RAG conversationnel avec mÃ©moire de conversation.
    
    GÃ¨re les questions de suivi en tenant compte de l'historique.
    
    Args:
        query: Question actuelle de l'utilisateur
        chat_history: Liste des messages prÃ©cÃ©dents [{"role": "user/assistant", "content": "..."}]
        
    Returns:
        str: RÃ©ponse avec sources
    """
    global vectorstore, llm

    if vectorstore is None or llm is None:
        return "âš ï¸ Components not initialized!"

    if chat_history is None:
        chat_history = []

    # 1. Reformuler la question en tenant compte du contexte conversationnel
    if chat_history:
        # Construire le contexte conversationnel
        conversation_context = "\n".join([
            f"{'Utilisateur' if msg['role'] == 'user' else 'Assistant'}: {msg['content']}"
            for msg in chat_history[-6:]  # Garder seulement les 6 derniers messages
        ])
        
        reformulation_prompt = f"""Contexte de conversation prÃ©cÃ©dente :
{conversation_context}

Question actuelle : {query}

Si la question actuelle fait rÃ©fÃ©rence Ã  un Ã©lÃ©ment de la conversation prÃ©cÃ©dente (ex: "Et pour les enfants ?", "Peux-tu prÃ©ciser ?", "Qu'en est-il de...", etc.), 
reformule-la en une question autonome complÃ¨te qui inclut le contexte nÃ©cessaire.

Si la question est dÃ©jÃ  autonome, retourne-la telle quelle.

Retourne UNIQUEMENT la question reformulÃ©e, sans explication."""

        reformulation_messages = [
            SystemMessage(content="Tu es un assistant qui reformule les questions pour les rendre autonomes."),
            HumanMessage(content=reformulation_prompt)
        ]
        
        reformulated = llm.invoke(reformulation_messages)
        search_query = reformulated.content.strip()
    else:
        search_query = query

    # 2. Recherche vectorielle avec la question (reformulÃ©e ou originale)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 10})
    docs = retriever.invoke(search_query)

    if not docs:
        return "âš ï¸ Aucun document pertinent trouvÃ©. Veuillez d'abord indexer des documents."

    # 3. Organiser les documents par source
    sources_dict = {}
    for doc in docs:
        source = doc.metadata.get("source", "Unknown")
        if source not in sources_dict:
            sources_dict[source] = []
        sources_dict[source].append(doc)

    # 4. CrÃ©er le contexte documentaire
    context_parts = []
    for source, source_docs in sources_dict.items():
        for idx, doc in enumerate(source_docs, 1):
            context_parts.append(
                f"[Document: {source} | Chunk {idx}]\n{doc.page_content[:500]}"
            )

    doc_context = "\n\n---\n\n".join(context_parts)

    # 5. Construire le prompt avec historique conversationnel
    system_prompt = """Tu es LuXas, un assistant juridique pÃ©dagogue spÃ©cialisÃ© dans les propositions de loi de l'AssemblÃ©e Nationale franÃ§aise.

INSTRUCTIONS CRITIQUES - ANTI-HALLUCINATION :
1. **Tu ne rÃ©ponds QUE si l'information est dans les documents fournis**
2. Si l'info n'est pas dans les documents, rÃ©ponds : "Je n'ai pas trouvÃ© cette information dans les documents indexÃ©s."
3. **JAMAIS d'invention** : ne crÃ©e pas de noms de loi, dates, ou articles qui ne sont pas dans les documents
4. Cite TOUJOURS les sources exactes (nom du document)
5. Si une question fait rÃ©fÃ©rence Ã  la conversation prÃ©cÃ©dente, utilise l'historique pour comprendre le contexte
6. Structure tes rÃ©ponses clairement et utilise un langage pÃ©dagogique
7. Si tu utilises des termes juridiques complexes, propose une dÃ©finition simple avec exemple

RÃˆGLE D'OR : En cas de doute, dis que tu n'as pas l'information plutÃ´t que d'inventer."""

    # Construire l'historique conversationnel pour le contexte
    conversation_context = ""
    if chat_history:
        conversation_context = "Historique de conversation :\n"
        for msg in chat_history[-4:]:  # Garder les 4 derniers Ã©changes
            role = "Utilisateur" if msg['role'] == 'user' else "Assistant"
            conversation_context += f"{role}: {msg['content'][:200]}...\n"
        conversation_context += "\n"

    user_prompt = f"""{conversation_context}Documents disponibles :

{doc_context}

Question actuelle : {query}

RÃ©ponds Ã  la question en te basant UNIQUEMENT sur les documents fournis. Cite tes sources."""

    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_prompt)
    ]

    # 6. GÃ©nÃ©rer la rÃ©ponse
    response = llm.invoke(messages)
    answer = response.content

    # 7. Ajouter les sources
    unique_sources = list(sources_dict.keys())
    source_count = len(unique_sources)
    chunk_count = len(docs)

    sources_section = f"\n\n{'='*60}\nðŸ“š **Sources ConsultÃ©es** ({source_count} document(s), {chunk_count} chunk(s))\n{'='*60}\n\n"

    for source, source_docs in sources_dict.items():
        sources_section += f"ðŸ“„ **{source}** ({len(source_docs)} chunk(s))\n"
        for idx, doc in enumerate(source_docs[:2], 1):
            preview = doc.page_content[:120].replace("\n", " ").strip()
            if len(doc.page_content) > 120:
                preview += "..."
            sources_section += f"   â€¢ Extrait {idx}: _{preview}_\n"
        if len(source_docs) > 2:
            sources_section += f"   â€¢ ... et {len(source_docs) - 2} autre(s) extrait(s)\n"
        sources_section += "\n"

    return f"{answer}{sources_section}"
