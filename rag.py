import os
from pathlib import Path
from typing import List
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_qdrant import QdrantVectorStore
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams
from utils import initialize_component

# Global variables
embeddings = None
vectorstore = None
llm = None
collection_name = "rag_documents"
# Chemin pour la persistance de Qdrant sur disque
QDRANT_PATH = "./data/qdrant_db"

def initialize_components():
    """
    Initialise les composants : embeddings, LLM, et Qdrant.
    
    Utilise Qdrant en mode PERSISTANT (sauvegarde automatique sur disque).
    Plus besoin de save_index() manuel !
    """
    global embeddings, llm, vectorstore

    if not os.getenv("OPENAI_API_KEY"):
        raise ValueError("OPENAI_API_KEY not found in environment variables!")

    # 1. Cr√©er les embeddings OpenAI
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    
    # 2. Cr√©er le LLM
    llm = initialize_component("LLM", {"model": "gpt-4", "temperature": 0})

    # 3. Cr√©er le client Qdrant en mode PERSISTANT (sur disque)
    os.makedirs(QDRANT_PATH, exist_ok=True)
    client = QdrantClient(path=QDRANT_PATH)  # Sauvegarde automatique !
    
    # 4. Cr√©er la collection si elle n'existe pas
    try:
        client.get_collection(collection_name)
        print(f"‚úÖ Collection '{collection_name}' existante charg√©e")
    except Exception:
        client.create_collection(
            collection_name=collection_name,
            vectors_config=VectorParams(
                size=1536,  # Dimension de text-embedding-3-small
                distance=Distance.COSINE
            )
        )
        print(f"‚úÖ Nouvelle collection '{collection_name}' cr√©√©e")

    # 5. Cr√©er le vectorstore LangChain
    vectorstore = QdrantVectorStore(
        client=client,
        collection_name=collection_name,
        embedding=embeddings
    )
    
    return "‚úÖ Components initialized successfully!"

def load_document(file_path: str) -> List[Document]:
    """Load a document based on its file extension"""
    file_ext = Path(file_path).suffix.lower()
    if file_ext == ".pdf":
        from langchain_community.document_loaders import PyPDFLoader
        loader = PyPDFLoader(file_path)
        return loader.load()
    else:
        raise ValueError(f"Unsupported file type: {file_ext}")

def index_documents(files: List[str]) -> str:
    """Index uploaded documents to Qdrant"""
    global vectorstore

    if vectorstore is None:
        return "‚ö†Ô∏è Components not initialized!"

    all_documents = []
    for file_path in files:
        docs = load_document(file_path)
        for doc in docs:
            doc.metadata["source"] = Path(file_path).name
        all_documents.extend(docs)

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=["\n\n", "\n", ". ", " ", ""]
    )
    chunks = text_splitter.split_documents(all_documents)
    vectorstore.add_documents(chunks)
    return f"‚úÖ Indexed {len(chunks)} chunks from {len(files)} files."

def rag_agent(query: str):
    """
    Effectue une recherche RAG simple et g√©n√®re une r√©ponse.
    
    NOTE: Cette fonction est simplifi√©e. Utilisez rag_agent_with_sources() 
    pour des r√©ponses plus d√©taill√©es avec citations.
    
    Args:
        query: Question de l'utilisateur
        
    Returns:
        str: R√©ponse g√©n√©r√©e
    """
    global vectorstore, llm

    if vectorstore is None or llm is None:
        return "‚ö†Ô∏è Components not initialized!"

    # 1. R√©cup√©rer les documents pertinents
    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
    docs = retriever.invoke(query)
    
    # 2. Cr√©er le contexte
    context = "\n".join([doc.page_content for doc in docs])

    # 3. Appeler le LLM avec LangChain (pas de format dict)
    from langchain_core.messages import SystemMessage, HumanMessage
    messages = [
        SystemMessage(content="Use the context below to answer the question."),
        HumanMessage(content=f"Context: {context}\n\nQuestion: {query}")
    ]
    response = llm.invoke(messages)
    return response.content

def rag_agent_with_sources(query: str):
    """
    Effectue une recherche RAG avec citations d√©taill√©es des sources.
    
    √âTAPES :
    1. Recherche les documents pertinents dans Qdrant
    2. Organise les documents par source
    3. G√©n√®re une r√©ponse avec le LLM
    4. Ajoute les citations de sources
    
    Args:
        query: Question de l'utilisateur
        
    Returns:
        str: R√©ponse avec citations des sources
    """
    global vectorstore, llm

    if vectorstore is None or llm is None:
        return "‚ö†Ô∏è Components not initialized!"

    # 1. R√©cup√©rer les documents pertinents
    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
    docs = retriever.invoke(query)

    if not docs:
        return "‚ö†Ô∏è No relevant documents found. Please index some documents first."

    # 2. Organiser les documents par source
    sources_dict = {}
    for doc in docs:
        source = doc.metadata.get("source", "Unknown")
        if source not in sources_dict:
            sources_dict[source] = []
        sources_dict[source].append(doc)

    # 3. Cr√©er le contexte avec attribution des sources
    context_parts = []
    for source, source_docs in sources_dict.items():
        for idx, doc in enumerate(source_docs, 1):
            context_parts.append(
                f"[Document: {source} | Chunk {idx}]\n{doc.page_content[:300]}"
            )

    context = "\n\n---\n\n".join(context_parts)

    # 4. Appeler le LLM avec LangChain
    from langchain_core.messages import SystemMessage, HumanMessage
    
    system_prompt = """Tu es un assistant qui r√©pond aux questions en te basant sur les documents fournis.

INSTRUCTIONS IMPORTANTES :
1. Synth√©tise les informations de TOUS les documents pertinents
2. Cite toujours le nom et la r√©f√©rence de la loi source si pr√©sente
3. Si la r√©ponse n'est pas dans le contexte, dis-le clairement
4. Sois clair et structur√© dans ta r√©ponse"""

    user_prompt = f"""Contexte des documents index√©s :

{context}

Question : {query}

Fournis une r√©ponse compl√®te en citant les documents sources."""

    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_prompt)
    ]

    # 5. G√©n√©rer la r√©ponse
    response = llm.invoke(messages)
    answer = response.content

    # 6. Ajouter les d√©tails des sources
    unique_sources = list(sources_dict.keys())
    source_count = len(unique_sources)
    chunk_count = len(docs)

    sources_section = f"\n\n{'='*60}\nüìö **Sources Utilis√©es** ({source_count} document(s), {chunk_count} chunk(s))\n{'='*60}\n\n"

    for source, source_docs in sources_dict.items():
        sources_section += f"üìÑ **{source}** ({len(source_docs)} chunk(s))\n"
        for idx, doc in enumerate(source_docs[:3], 1):
            preview = doc.page_content[:150].replace("\n", " ").strip()
            if len(doc.page_content) > 150:
                preview += "..."
            sources_section += f"   ‚Ä¢ Chunk {idx}: _{preview}_\n"
        if len(source_docs) > 3:
            sources_section += f"   ‚Ä¢ ... et {len(source_docs) - 3} chunk(s) de plus\n"
        sources_section += "\n"

    return f"{answer}{sources_section}"

def rag_agent_with_metadata(query: str):
    """
    Reformule une question et g√©n√®re des m√©tadonn√©es enrichies, y compris un titre bas√© sur 'Proposition de loi'.

    Args:
        query (str): Question utilisateur.

    Returns:
        dict: Contexte reformul√© et m√©tadonn√©es enrichies.
    """
    global vectorstore, llm

    if vectorstore is None or llm is None:
        return {"error": "‚ö†Ô∏è Components not initialized!"}

    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
    docs = retriever.invoke(query)

    if not docs:
        return {"error": "‚ö†Ô∏è No relevant documents found. Please index some documents first."}

    # G√©n√©rer un titre bas√© sur le contenu des documents
    title = "Proposition de loi : " + (docs[0].metadata.get("title") or "Titre inconnu")

    # Cr√©er un contexte √† partir des documents
    context = "\n".join([doc.page_content for doc in docs])

    return {
        "title": title,
        "context": context,
        "documents": docs
    }

def train_rag_with_pdfs(pdf_folder: str):
    """
    Entra√Æner le RAG avec les fichiers PDF d'un dossier donn√©.

    Args:
        pdf_folder (str): Chemin vers le dossier contenant les fichiers PDF.

    Returns:
        str: R√©sultat de l'indexation.
    """
    global vectorstore

    if vectorstore is None:
        return "‚ö†Ô∏è Components not initialized!"

    from pathlib import Path

    pdf_files = list(Path(pdf_folder).glob("*.pdf"))
    if not pdf_files:
        return "‚ö†Ô∏è Aucun fichier PDF trouv√© dans le dossier sp√©cifi√©."

    all_documents = []
    for pdf_file in pdf_files:
        docs = load_document(str(pdf_file))
        for doc in docs:
            doc.metadata["source"] = pdf_file.name
        all_documents.extend(docs)

    # D√©couper les documents en chunks
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=["\n\n", "\n", ". ", " ", ""]
    )
    chunks = text_splitter.split_documents(all_documents)

    # Ajouter les chunks au magasin vectoriel
    vectorstore.add_documents(chunks)
    return f"‚úÖ Indexation termin√©e : {len(chunks)} chunks ajout√©s √† partir de {len(pdf_files)} fichiers PDF."

def save_index(index_path: str = None):
    """
    FONCTION OBSOL√àTE - Plus n√©cessaire !
    
    Avec Qdrant en mode persistant (path=QDRANT_PATH), 
    l'index est AUTOMATIQUEMENT sauvegard√© sur disque.
    
    Cette fonction ne fait rien mais reste pour la compatibilit√©.
    """
    global vectorstore
    if vectorstore is None:
        return "‚ö†Ô∏è Vectorstore not initialized!"
    
    # Qdrant sauvegarde automatiquement, rien √† faire !
    return f"‚úÖ Index d√©j√† sauvegard√© automatiquement dans {QDRANT_PATH}"

def load_index(index_path: str = None):
    """
    FONCTION OBSOL√àTE - Plus n√©cessaire !
    
    Avec Qdrant en mode persistant, l'index est AUTOMATIQUEMENT charg√©
    au d√©marrage via initialize_components().
    
    Cette fonction ne fait rien mais reste pour la compatibilit√©.
    """
    return f"‚úÖ Index d√©j√† charg√© automatiquement depuis {QDRANT_PATH}"

def clear_index() -> str:
    """
    R√©initialise compl√®tement l'index vectoriel.
    
    ATTENTION : Cela supprime TOUS les documents index√©s !
    """
    global vectorstore, embeddings

    if vectorstore is None:
        return "‚ö†Ô∏è Vectorstore not initialized!"

    try:
        # 1. R√©cup√©rer le client Qdrant
        client = vectorstore.client
        
        # 2. Supprimer la collection existante
        try:
            client.delete_collection(collection_name)
            print(f"üóëÔ∏è Collection '{collection_name}' supprim√©e")
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors de la suppression : {e}")

        # 3. Recr√©er une collection vide
        client.create_collection(
            collection_name=collection_name,
            vectors_config=VectorParams(
                size=1536,
                distance=Distance.COSINE
            )
        )
        print(f"‚úÖ Nouvelle collection '{collection_name}' cr√©√©e")

        # 4. Recr√©er le vectorstore
        vectorstore = QdrantVectorStore(
            client=client,
            collection_name=collection_name,
            embedding=embeddings
        )

        return "‚úÖ Index r√©initialis√© avec succ√®s !"
    except Exception as e:
        return f"‚ùå Erreur lors de la r√©initialisation : {str(e)}"

