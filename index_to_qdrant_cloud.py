"""
Script pour indexer les PDFs locaux vers Qdrant Cloud.

Permet de migrer votre base locale vers le cloud pour :
- Ã‰viter les problÃ¨mes de RAM Docker
- AccÃ¨s depuis n'importe oÃ¹
- Performance optimale
"""

import os
from pathlib import Path
from tqdm import tqdm
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_qdrant import QdrantVectorStore
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams
from config import OPENAI_API_KEY, QDRANT_API_KEY, QDRANT_CLOUD_URL

# Configuration Qdrant Cloud
COLLECTION_NAME = "rag_documents"

# Configuration
PDF_FOLDER = "db_local_pdfs"
MAX_CHUNKS_PER_BATCH = 5000  # Limite par nombre de chunks au lieu de PDFs
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 200

def index_pdfs_to_cloud():
    """
    Indexe tous les PDFs du dossier local vers Qdrant Cloud.
    """
    print("=" * 80)
    print("ğŸš€ INDEXATION VERS QDRANT CLOUD")
    print("=" * 80)
    
    
    # 1. CrÃ©er le client Qdrant Cloud
    print(f"\nğŸŒ Connexion Ã  Qdrant Cloud : {QDRANT_CLOUD_URL}")
    client = QdrantClient(
        url=QDRANT_CLOUD_URL,
        api_key=QDRANT_API_KEY,
    )
    
    # 2. CrÃ©er les embeddings
    print("ğŸ”§ Initialisation des embeddings OpenAI...")
    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small",
        openai_api_key=OPENAI_API_KEY
    )
    
    # 3. CrÃ©er ou rÃ©cupÃ©rer la collection
    try:
        client.get_collection(COLLECTION_NAME)
        print(f"âœ… Collection '{COLLECTION_NAME}' existante trouvÃ©e")
    except Exception:
        print(f"ğŸ“¦ CrÃ©ation de la collection '{COLLECTION_NAME}'...")
        client.create_collection(
            collection_name=COLLECTION_NAME,
            vectors_config=VectorParams(
                size=1536,
                distance=Distance.COSINE
            )
        )
        print(f"âœ… Collection crÃ©Ã©e")
    
    # 4. CrÃ©er le vectorstore
    vectorstore = QdrantVectorStore(
        client=client,
        collection_name=COLLECTION_NAME,
        embedding=embeddings
    )
    
    # 5. Lister les PDFs
    pdf_folder = Path(PDF_FOLDER)
    if not pdf_folder.exists():
        print(f"âŒ Le dossier '{PDF_FOLDER}' n'existe pas!")
        return
    
    pdf_files = list(pdf_folder.glob("*.pdf"))
    total_pdfs = len(pdf_files)
    
    if total_pdfs == 0:
        print(f"âŒ Aucun PDF trouvÃ© dans '{PDF_FOLDER}'")
        return
    
    print(f"\nğŸ“š {total_pdfs} PDFs Ã  indexer")
    
    # 6. Initialiser le text splitter
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=["\n\n", "\n", ". ", " ", ""]
    )
    
    # 7. Traiter PDF par PDF, grouper les chunks par batches
    total_chunks_indexed = 0
    failed_files = []
    current_batch_chunks = []
    batch_num = 1
    pdfs_processed = 0
    
    print(f"\nğŸ“Š StratÃ©gie : batches de maximum {MAX_CHUNKS_PER_BATCH} chunks")
    
    for pdf_file in tqdm(pdf_files, desc="Indexation", unit="PDF"):
        try:
            # Charger le PDF
            loader = PyPDFLoader(str(pdf_file))
            docs = loader.load()
            
            # Ajouter les mÃ©tadonnÃ©es
            for doc in docs:
                doc.metadata["source"] = pdf_file.name
            
            # DÃ©couper en chunks
            pdf_chunks = text_splitter.split_documents(docs)
            
            # Ajouter les chunks au batch actuel
            current_batch_chunks.extend(pdf_chunks)
            pdfs_processed += 1
            
            # Si on dÃ©passe la limite, uploader le batch
            if len(current_batch_chunks) >= MAX_CHUNKS_PER_BATCH:
                print(f"\nâ˜ï¸ Upload batch {batch_num} : {len(current_batch_chunks)} chunks ({pdfs_processed} PDFs)")
                try:
                    vectorstore.add_documents(current_batch_chunks)
                    total_chunks_indexed += len(current_batch_chunks)
                    print(f"âœ… Batch {batch_num} indexÃ© avec succÃ¨s")
                except Exception as e:
                    print(f"âŒ Erreur lors de l'indexation du batch {batch_num}: {e}")
                
                # RÃ©initialiser pour le prochain batch
                current_batch_chunks = []
                batch_num += 1
                pdfs_processed = 0
                
        except Exception as e:
            print(f"\nâš ï¸ Erreur avec {pdf_file.name}: {e}")
            failed_files.append(str(pdf_file))
    
    # Uploader le dernier batch s'il reste des chunks
    if current_batch_chunks:
        print(f"\nâ˜ï¸ Upload batch final {batch_num} : {len(current_batch_chunks)} chunks ({pdfs_processed} PDFs)")
        try:
            vectorstore.add_documents(current_batch_chunks)
            total_chunks_indexed += len(current_batch_chunks)
            print(f"âœ… Batch {batch_num} indexÃ© avec succÃ¨s")
        except Exception as e:
            print(f"âŒ Erreur lors de l'indexation du batch final {batch_num}: {e}")
    
    total_chunks = total_chunks_indexed
    
    # 8. RÃ©sumÃ© final
    print("\n" + "=" * 80)
    print("âœ… INDEXATION TERMINÃ‰E")
    print("=" * 80)
    print(f"ğŸ“„ PDFs traitÃ©s : {total_pdfs - len(failed_files)}/{total_pdfs}")
    print(f"ğŸ“¦ Chunks indexÃ©s : {total_chunks}")
    print(f"â˜ï¸ Base vectorielle : Qdrant Cloud")
    print(f"ğŸŒ URL : {QDRANT_CLOUD_URL}")
    
    if failed_files:
        print(f"\nâš ï¸ {len(failed_files)} fichiers ont Ã©chouÃ© :")
        for failed in failed_files[:10]:
            print(f"   - {failed}")
        if len(failed_files) > 10:
            print(f"   ... et {len(failed_files) - 10} autres")
    
    print("\nğŸ‰ Votre systÃ¨me RAG est maintenant dans le cloud !")
    print("=" * 80)

if __name__ == "__main__":
    index_pdfs_to_cloud()
